
\subsection{Differential Privacy}
Differential privacy is a security definition for the release of information about a database of private records. Here we outline the foundational definitions and theorems in differential privacy; everything below first appeared in the seminal paper of Dwork et al.~\cite{dwork2006calibrating}.

Suppose we have a database \x containing sensitive information that we want to study. In particular, we want to publish the output $f(\x)$ of a function $f$ (also sometimes called a \textit{mechanism}) on our database while protecting the privacy of the individuals whose data was collected. Differential privacy promises that an adversary will learn approximately nothing about an individual as a result of their presence in \x.  Informally, this is done by requiring that the probability of seeing any particular output is roughly the same regardless of what information a given individual submitted to the database.

As above, we use \dbsize to represent the number of rows in \x, where a ``row'' is simply the set of data associated with a single individual.  We now define \textit{neighboring databases}, which differ in only one row.

\begin{definition}[Neighboring Databases]\label{def:Neighboring} Two databases \x and $\x'$ are \textit{neighboring} if \x can be transformed to $\x'$ by changing only one individual's data (where a change is an in-place modification, not a full addition or removal).
\end{definition}

To protect the privacy of individuals in a database, differentially private requires that the output of a query on any two neighboring databases should look nearly identical.

\begin{definition}[Differential Privacy] \label{def:diffpriv}
A (randomized) mechanism $f$ with range R is \emph{$\eps$-differentially private} if for all $S \subseteq R$ and for all neighboring databases \x and $\x'$
\begin{equation*}
\text{\emph{Pr}}[f(\x) \in S] \leq e^{\eps} \text{\emph{Pr}}[f(\x') \in S]. 
\end{equation*}
\end{definition}

The parameter \eps is called the \textit{privacy parameter}, and its choice is a policy decision. The lower the chosen \eps, the stronger the privacy guarantee. Note that because neighboring databases are the same size, $N$ can always be released without compromising privacy.\footnote{Differential privacy can also be defined in terms of databases that differ by an addition/deletion, rather than by a change in a row.  For most applications these definitions are equivalent except for a change in $\epsilon$ by a factor of two (with the version here being the more stringent interpretation of $\epsilon$). The one significant difference is that the other definition does not result in $N$ being public, which is important for our work here.}

Differential privacy has several useful properties.  One of the most useful is composition:

\begin{theorem}[Composition]\label{thm:composition} Suppose $f$ and $g$ are respectively $\eps_{1}$- and $\eps_{2}$-differentially private mechanisms. Then, a mechanism $h$ that returns the results of applying $f$ and $g$ to \x, $h(\x) = (f(\x), g(\x))$, is $(\eps_{1} + \eps_{2})$-differentially private.
\end{theorem}

In other words, the privacy guarantee decreases, but does not disappear, when a database is queried multiple times.  Composition allows database administrators to issue researchers a privacy budget, which researchers can then divide up as they wish between any number of different queries.

Another defining feature of differential privacy is its resistance to \textit{post-processing}.

\begin{theorem}[Post-Processing] \label{thm:postprocessing}
Let $f$ be an $\eps$-differentially private mechanism, and let $g$ be an arbitrary function. Then, $h(\x) = g(f(\x))$ is also $\eps$-differentially private.
\end{theorem}

This theorem allows us to do any computation we desire on the output of our differentially-private mechanism without diminishing the privacy guarantees. We will utilize this property to compute $p$-values of the private $F$-statistic.  The $p$-values will be automatically private without additional argument.

Our algorithms are constructed by taking building blocks and combining them with composition and post-processing, but the fundamental building blocks are made private using the Laplace mechanism, the oldest and maybe simplest method for achieving differential privacy.  The Laplace mechanism allows the conversion of any function $f$ into a private approximation $\hat{f}$. One must first compute (or bound) the \textit{sensitivity} of the function, the maximum effect on the output that a single row can have.

\begin{definition}[Sensitivity] \label{def:sensitivity}
The sensitivity of a (deterministic) real-valued function $f$ on databases is the maximum of  $\lvert f(\x) - f(\x') \rvert$ taken over all pairs $(\x, \x')$ of neighboring databases.\footnote{Sensitivity and the Laplace mechanism can be defined on functions with output in $\mathbb{R}^n$, but we only need the one-dimensional version.}
\end{definition}

The Laplace mechanism will use random noise drawn from the Laplace distribution.

\begin{definition}[Laplace Distribution] \label{def:laplacedist}
The Laplace Distribution (centered at 0) with scale $b$ is the distribution with probability density function
\begin{equation*}
\lap(z\mid b) = \frac{1}{2b}\text{\emph{exp}}\bigg({-\frac{\lvert z \rvert}{b}}\bigg).
\end{equation*}
We use $\lap(b)$ to represent the Laplace distribution with scale $b$.
\end{definition}

We can now present the Laplace mechanism.

\begin{theorem}[Laplacian Mechanism] \label{thm:lapmechanism}
Let $f$ be a function with sensitivity bounded above by $s$. Let $L$ be a random variable drawn from $\lap(s/\eps)$. Then the function $\hat{f}(\x) = f(\x) + L$ is \eps-differentially private. 
\end{theorem}